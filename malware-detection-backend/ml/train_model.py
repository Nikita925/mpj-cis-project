import pickle
import random
import numpy as np
import math
import os
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC

# Calculates the entropy of file content
def calculate_entropy(file_data):
    byte_frequencies = [file_data.count(chr(i)) / len(file_data) for i in range(256)]
    entropy = -sum([f * math.log2(f) for f in byte_frequencies if f > 0])
    return entropy

# Extracts the file extension
def extract_extension(file_name):
    return file_name.split('.')[-1] if '.' in file_name else 'unknown'

# Counts suspicious strings in the file content
def count_suspicious_strings(file_data):
    suspicious_strings = ['cmd', 'powershell', 'netstat', 'ftp', 'wget', 'eval']
    count = sum([file_data.lower().count(s) for s in suspicious_strings])
    return count

# Lists to hold features (X) and labels (y)
X = []
y = []

# Generate a synthetic dataset
for _ in range(500):
    size = random.randint(100, 5000)  # Random file size
    specials = random.randint(0, size // 10)  # Random number of special characters

    # Randomly generate a file name with a common extension
    file_name = f"file{random.randint(1, 100)}.{random.choice(['exe', 'txt', 'bat', 'js', 'pdf'])}"
    
    # Randomly generate file content (simulate real file data)
    file_data = ''.join(random.choices('abcdefghijklmnopqrstuvwxyz0123456789', k=size))

    # Extract features from the generated data
    entropy = calculate_entropy(file_data)
    extension = extract_extension(file_name)
    suspicious_strings_count = count_suspicious_strings(file_data)

    # Append features
    X.append([size, specials, entropy, suspicious_strings_count, len(file_name), len(extension)])
    
    # Label: 1 = malware, 0 = benign
    y.append(1 if specials > (size * 0.05) or suspicious_strings_count > 2 else 0)

X = np.array(X)
y = np.array(y)

# Define machine learning models
models = {
    "logistic_regression": LogisticRegression(),
    "decision_tree": DecisionTreeClassifier(),
    "random_forest": RandomForestClassifier(),
    "knn": KNeighborsClassifier(),
    "svm": SVC(probability=True)
}

# Train each model and save it as a .pkl file
for name, model in models.items():
    model.fit(X, y)
    with open(f'{name}_model.pkl', 'wb') as f:
        pickle.dump(model, f)

print("âœ… All models trained and saved!")
